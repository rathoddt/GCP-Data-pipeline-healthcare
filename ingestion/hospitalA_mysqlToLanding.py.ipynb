{"cells": [{"cell_type": "code", "execution_count": 41, "id": "f826f9a5-98df-475d-b05c-ab71282d0400", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:07.977114] INFO - \u2705 Successfully read the config file\n[2026-01-16T08:48:08.537314] INFO - Moved landing/hospital-a/encounters/encounters_16012026.json to landing/hospital-a/archive/encounters/2026/01/16/encounters_16012026.json\n[2026-01-16T08:48:09.468282] INFO - Latest watermark for encounters: 1900-01-01 00:00:00\n[2026-01-16T08:48:09.515090] SUCCESS - \u2705 Successfully extracted data from encounters\n[2026-01-16T08:48:10.202256] SUCCESS - \u2705 JSON file successfully written to gs://test-project-1857-de/landing/hospital-a/encounters/encounters_16012026.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:18.574601] SUCCESS - \u2705 Audit log updated for encounters\n[2026-01-16T08:48:18.917212] INFO - Moved landing/hospital-a/patients/patients_16012026.json to landing/hospital-a/archive/patients/2026/01/16/patients_16012026.json\n[2026-01-16T08:48:19.558869] INFO - Latest watermark for patients: 1900-01-01 00:00:00\n[2026-01-16T08:48:19.599528] SUCCESS - \u2705 Successfully extracted data from patients\n[2026-01-16T08:48:20.136785] SUCCESS - \u2705 JSON file successfully written to gs://test-project-1857-de/landing/hospital-a/patients/patients_16012026.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:26.587449] SUCCESS - \u2705 Audit log updated for patients\n[2026-01-16T08:48:26.926399] INFO - Moved landing/hospital-a/transactions/transactions_16012026.json to landing/hospital-a/archive/transactions/2026/01/16/transactions_16012026.json\n[2026-01-16T08:48:27.509417] INFO - Latest watermark for transactions: 1900-01-01 00:00:00\n[2026-01-16T08:48:27.539990] SUCCESS - \u2705 Successfully extracted data from transactions\n[2026-01-16T08:48:28.711869] SUCCESS - \u2705 JSON file successfully written to gs://test-project-1857-de/landing/hospital-a/transactions/transactions_16012026.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:35.037513] SUCCESS - \u2705 Audit log updated for transactions\n[2026-01-16T08:48:35.379835] INFO - Moved landing/hospital-a/providers/providers_16012026.json to landing/hospital-a/archive/providers/2026/01/16/providers_16012026.json\n[2026-01-16T08:48:35.380041] INFO - Latest watermark for providers: None\n[2026-01-16T08:48:35.410394] SUCCESS - \u2705 Successfully extracted data from providers\n[2026-01-16T08:48:35.613147] SUCCESS - \u2705 JSON file successfully written to gs://test-project-1857-de/landing/hospital-a/providers/providers_16012026.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:41.834801] SUCCESS - \u2705 Audit log updated for providers\n[2026-01-16T08:48:42.165934] INFO - Moved landing/hospital-a/departments/departments_16012026.json to landing/hospital-a/archive/departments/2026/01/16/departments_16012026.json\n[2026-01-16T08:48:42.166105] INFO - Latest watermark for departments: None\n[2026-01-16T08:48:42.200072] SUCCESS - \u2705 Successfully extracted data from departments\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:45.467480] SUCCESS - \u2705 JSON file successfully written to gs://test-project-1857-de/landing/hospital-a/departments/departments_16012026.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2026-01-16T08:48:55.860333] SUCCESS - \u2705 Audit log updated for departments\n\u2705 Logs successfully saved to GCS at gs://test-project-1857-de/temp/pipeline_logs/pipeline_log_20260116084855.json\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 Logs stored in BigQuery for future analysis\n"}], "source": "from google.cloud import storage, bigquery\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nimport datetime\nimport json\n\n# Initialize GCS & BigQuery Clients\nstorage_client = storage.Client()\nbq_client = bigquery.Client()\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"HospitalAMySQLToLanding\").getOrCreate()\n\n# Google Cloud Storage (GCS) Configuration\nGCS_BUCKET = \"test-project-1857-de\"\nHOSPITAL_NAME = \"hospital-a\"\nLANDING_PATH = f\"gs://{GCS_BUCKET}/landing/{HOSPITAL_NAME}/\"\nARCHIVE_PATH = f\"gs://{GCS_BUCKET}/landing/{HOSPITAL_NAME}/archive/\"\nCONFIG_FILE_PATH = f\"gs://{GCS_BUCKET}/configs/load_config.csv\"\n\n# BigQuery Configuration\nBQ_PROJECT = \"test-project-1857\"\nBQ_AUDIT_TABLE = f\"{BQ_PROJECT}.temp_dataset.audit_log\"\nBQ_LOG_TABLE = f\"{BQ_PROJECT}.temp_dataset.pipeline_logs\"\nBQ_TEMP_PATH = f\"{GCS_BUCKET}/temp/\"  \n\n# MySQL Configuration\n# MYSQL_CONFIG = {\n#     \"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db?useSSL=false&allowPublicKeyRetrieval=true\",\n#     \"driver\": \"com.mysql.cj.jdbc.Driver\",\n#     \"user\": \"user1\",\n#     \"password\": \"User1-1234\"\n# }\n\nMYSQL_CONFIG = {\n\"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\",\n\"driver\": \"com.mysql.cj.jdbc.Driver\",\n\"user\": \"user1\",\n\"password\": \"User1-1234\",\n\"socketFactory\": \"com.google.cloud.sql.mysql.SocketFactory\",\n\"cloudSqlInstance\": \"test-project-1857:us-central1:hospital-a-mysql-db\",\n\"useSSL\": \"false\",\n \"database\": \"hospital_a_db\"    \n}\n\n##------------------------------------------------------------------------------------------------------------------##\n# Logging Mechanism\nlog_entries = []  # Stores logs before writing to GCS\n\ndef log_event(event_type, message, table=None):\n    \"\"\"Log an event and store it in the log list\"\"\"\n    log_entry = {\n        \"timestamp\": datetime.datetime.now().isoformat(),\n        \"event_type\": event_type,\n        \"message\": message,\n        \"table\": table\n    }\n    log_entries.append(log_entry)\n    print(f\"[{log_entry['timestamp']}] {event_type} - {message}\")  # Print for visibility\n    \ndef save_logs_to_gcs():\n    \"\"\"Save logs to a JSON file and upload to GCS\"\"\"\n    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n    log_filepath = f\"temp/pipeline_logs/{log_filename}\"  \n    \n    json_data = json.dumps(log_entries, indent=4)\n\n    # Get GCS bucket\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blob = bucket.blob(log_filepath)\n    \n    # Upload JSON data as a file\n    blob.upload_from_string(json_data, content_type=\"application/json\")\n\n    print(f\"\u2705 Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n\ndef save_logs_to_bigquery():\n    \"\"\"Save logs to BigQuery\"\"\"\n    if log_entries:\n        log_df = spark.createDataFrame(log_entries)\n        log_df.write.format(\"bigquery\") \\\n            .option(\"table\", BQ_LOG_TABLE) \\\n            .option(\"temporaryGcsBucket\", BQ_TEMP_PATH) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"\u2705 Logs stored in BigQuery for future analysis\")\n    \n##------------------------------------------------------------------------------------------------------------------##\n\n# Function to Move Existing Files to Archive\ndef move_existing_files_to_archive(table):\n    blobs = list(storage_client.bucket(GCS_BUCKET).list_blobs(prefix=f\"landing/{HOSPITAL_NAME}/{table}/\"))\n    existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n\n    if not existing_files:\n        log_event(\"INFO\", f\"No existing files for table {table}\")\n        return\n\n    for file in existing_files:\n        source_blob = storage_client.bucket(GCS_BUCKET).blob(file)\n\n        # Extract Date from File Name\n        date_part = file.split(\"_\")[-1].split(\".\")[0]\n        year, month, day = date_part[-4:], date_part[2:4], date_part[:2]\n\n        # Move to Archive\n        archive_path = f\"landing/{HOSPITAL_NAME}/archive/{table}/{year}/{month}/{day}/{file.split('/')[-1]}\"\n        destination_blob = storage_client.bucket(GCS_BUCKET).blob(archive_path)\n\n        # Copy file to archive and delete original\n        storage_client.bucket(GCS_BUCKET).copy_blob(source_blob, storage_client.bucket(GCS_BUCKET), destination_blob.name)\n        source_blob.delete()\n\n        log_event(\"INFO\", f\"Moved {file} to {archive_path}\", table=table)\n        \n##------------------------------------------------------------------------------------------------------------------##\n\n# Function to Get Latest Watermark from BigQuery Audit Table\ndef get_latest_watermark(table_name):\n    query = f\"\"\"\n        SELECT MAX(load_timestamp) AS latest_timestamp\n        FROM `{BQ_AUDIT_TABLE}`\n        WHERE tablename = '{table_name}' and data_source = \"hospital_a_db\"\n    \"\"\"\n    query_job = bq_client.query(query)\n    result = query_job.result()\n    for row in result:\n        return row.latest_timestamp if row.latest_timestamp else \"1900-01-01 00:00:00\"\n    return \"1900-01-01 00:00:00\"\n\n##------------------------------------------------------------------------------------------------------------------##\n\n# Function to Extract Data from MySQL and Save to GCS\ndef extract_and_save_to_landing(table, load_type, watermark_col):\n    try:\n        last_watermark = get_latest_watermark(table) if load_type.lower() == \"incremental\" else None\n        log_event(\"INFO\", f\"Latest watermark for {table}: {last_watermark}\", table=table)\n\n        query = f\"(SELECT * FROM {table}) AS t\" if load_type.lower() == \"full\" else \\\n                f\"(SELECT * FROM {table} WHERE {watermark_col} > '{last_watermark}') AS t\"\n\n        df = (spark.read.format(\"jdbc\")\n                .option(\"url\", MYSQL_CONFIG[\"url\"])\n                .option(\"user\", MYSQL_CONFIG[\"user\"])\n                .option(\"password\", MYSQL_CONFIG[\"password\"])\n                .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n                .option(\"dbtable\", query)\n                .option(\"database\", MYSQL_CONFIG[\"database\"])\n                .load())\n#         df = spark.read.format(\"jdbc\") \\\n#         .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n#         .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n#         .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n#         .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n#         .option(\"dbtable\", query) \\\n#         .option(\"database\", MYSQL_CONFIG[\"database\"]) \\\n#         .load()\n\n        log_event(\"SUCCESS\", f\"\u2705 Successfully extracted data from {table}\", table=table)\n\n        today = datetime.datetime.today().strftime('%d%m%Y')\n        JSON_FILE_PATH = f\"landing/{HOSPITAL_NAME}/{table}/{table}_{today}.json\"\n\n        bucket = storage_client.bucket(GCS_BUCKET)\n        blob = bucket.blob(JSON_FILE_PATH)\n        blob.upload_from_string(df.toPandas().to_json(orient=\"records\", lines=True), content_type=\"application/json\")\n\n        log_event(\"SUCCESS\", f\"\u2705 JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n        \n        # Insert Audit Entry\n        audit_df = spark.createDataFrame([\n            (\"hospital_a_db\", table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")], \n            [\"data_source\", \"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n\n        (audit_df.write.format(\"bigquery\")\n            .option(\"table\", BQ_AUDIT_TABLE)\n            .option(\"temporaryGcsBucket\", GCS_BUCKET)\n            .mode(\"append\")\n            .save())\n\n        log_event(\"SUCCESS\", f\"\u2705 Audit log updated for {table}\", table=table)\n\n    except Exception as e:\n        log_event(\"ERROR\", f\"Error processing {table}: {str(e)}\", table=table)\n##------------------------------------------------------------------------------------------------------------------##\n\n# Function to Read Config File from GCS\ndef read_config_file():\n    df = spark.read.csv(CONFIG_FILE_PATH, header=True)\n    log_event(\"INFO\", \"\u2705 Successfully read the config file\")\n    return df\n\n# read config file\nconfig_df = read_config_file()\n\nfor row in config_df.collect():\n    if row[\"is_active\"] == '1' and row[\"datasource\"] == \"hospital_a_db\": \n        db, src, table, load_type, watermark, _, targetpath = row\n        move_existing_files_to_archive(table)\n        extract_and_save_to_landing(table, load_type, watermark)\n        \nsave_logs_to_gcs()\nsave_logs_to_bigquery()"}, {"cell_type": "code", "execution_count": null, "id": "82bc195d-3b75-496b-bd52-5c760ebd93c0", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 6, "id": "c781de05-75af-4426-90b1-c6ac2c52ac6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "35.226.25.250"}], "source": "!curl ifconfig.me"}, {"cell_type": "code", "execution_count": 8, "id": "0d8eb7ca-3455-43b8-a3fd-0b5244540c7d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+\n|  1|\n+---+\n|  1|\n+---+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.read.format(\"jdbc\") \\\n  .option(\"url\", \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\") \\\n  .option(\"user\", \"user1\") \\\n  .option(\"password\", \"User1-1234\") \\\n  .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n  .option(\"query\", \"SELECT 1\") \\\n  .load().show()"}, {"cell_type": "code", "execution_count": 11, "id": "2965fa2a-47ab-4743-bf47-b6d4bcf21c9b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "mysql: [Warning] Using a password on the command line interface can be insecure.\n+---+\n| 1 |\n+---+\n| 1 |\n+---+\n"}], "source": "!mysql -h 34.60.182.151 -u user1 -pUser1-1234 -e \"select 1;\""}, {"cell_type": "code", "execution_count": 28, "id": "36965f6c-f914-4ee0-ad59-812cccba0071", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+\n|CURRENT_USER()|\n+--------------+\n|       user1@%|\n+--------------+\n\n"}], "source": "# MySQL Configuration\nMYSQL_CONFIG = {\n#     \"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db?useSSL=false&allowPublicKeyRetrieval=true\",\n    \"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\",\n    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n    \"user\": \"user1\",\n    \"password\": \"User1-1234\"\n}\ndf = spark.read.format(\"jdbc\") \\\n        .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n        .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n        .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n        .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n        .option(\"query\", \"SELECT CURRENT_USER()\") \\\n        .load()\ndf.show()        "}, {"cell_type": "code", "execution_count": 12, "id": "ecbed1e2-fe45-4dd5-99c4-0ee538500b08", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+\n|CURRENT_USER()|\n+--------------+\n|       user1@%|\n+--------------+\n\n"}], "source": "df = spark.read.format(\"jdbc\") \\\n.option(\"url\", \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\") \\\n.option(\"user\", \"user1\") \\\n.option(\"password\", \"User1-1234\") \\\n.option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n.option(\"query\", \"SELECT CURRENT_USER()\") \\\n.load()\n\ndf.show()\n"}, {"cell_type": "code", "execution_count": 31, "id": "1a7dfa1a-3f4c-45bf-9ccd-593a71eadbf8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+\n|CURRENT_USER()|\n+--------------+\n|       user1@%|\n+--------------+\n\n"}], "source": "MYSQL_CONFIG = {\n\"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\",\n\"driver\": \"com.mysql.cj.jdbc.Driver\",\n\"user\": \"user1\",\n\"password\": \"User1-1234\",\n\"socketFactory\": \"com.google.cloud.sql.mysql.SocketFactory\",\n\"cloudSqlInstance\": \"test-project-1857:us-central1:hospital-a-mysql-db\",\n\"useSSL\": \"false\"\n}\n\ndf = spark.read.format(\"jdbc\") \\\n        .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n        .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n        .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n        .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n        .option(\"query\", \"SELECT CURRENT_USER()\") \\\n        .load()\ndf.show()  "}, {"cell_type": "code", "execution_count": 38, "id": "1b3e66cd-a82a-40ee-97d7-ea4a8afbb24f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+------------+-------------+---------------+----------+------------+-------------+------------+------------+\n|EncounterID|   PatientID|EncounterDate|  EncounterType|ProviderID|DepartmentID|ProcedureCode|InsertedDate|ModifiedDate|\n+-----------+------------+-------------+---------------+----------+------------+-------------+------------+------------+\n|  ENC000001|HOSP1-001127|   2020-01-14|      Inpatient|  PROV0133|     DEPT019|        97099|  2023-03-08|  2020-03-01|\n|  ENC000002|HOSP1-003842|   2021-01-21|     Outpatient|  PROV0147|     DEPT013|        19272|  2022-05-01|  2022-02-05|\n|  ENC000003|HOSP1-001372|   2021-10-29|   Telemedicine|  PROV0444|     DEPT004|        65512|  2023-06-13|  2022-08-11|\n|  ENC000004|HOSP1-002649|   2023-05-05|Routine Checkup|  PROV0169|     DEPT010|        38334|  2020-06-14|  2023-06-30|\n|  ENC000005|HOSP1-001709|   2020-04-05|Routine Checkup|  PROV0479|     DEPT014|        10594|  2021-02-08|  2020-10-19|\n|  ENC000006|HOSP1-004896|   2020-11-10|Routine Checkup|  PROV0265|     DEPT018|        62851|  2024-08-21|  2021-07-02|\n|  ENC000007|HOSP1-004391|   2023-08-30|      Emergency|  PROV0204|     DEPT015|        51092|  2023-08-04|  2024-01-19|\n|  ENC000008|HOSP1-004169|   2021-04-02|      Emergency|  PROV0137|     DEPT004|        72519|  2022-11-22|  2021-08-04|\n|  ENC000009|HOSP1-003537|   2024-07-17|      Inpatient|  PROV0312|     DEPT012|        83568|  2021-11-12|  2021-11-13|\n|  ENC000010|HOSP1-002629|   2020-02-12|      Inpatient|  PROV0073|     DEPT003|        31703|  2021-09-01|  2022-03-16|\n|  ENC000011|HOSP1-000727|   2023-02-24|      Emergency|  PROV0083|     DEPT015|        95323|  2021-02-16|  2021-12-06|\n|  ENC000012|HOSP1-003425|   2020-03-09|   Telemedicine|  PROV0326|     DEPT006|        83138|  2023-07-20|  2020-06-08|\n|  ENC000013|HOSP1-003753|   2022-09-24|Routine Checkup|  PROV0426|     DEPT016|        35352|  2021-09-02|  2021-06-26|\n|  ENC000014|HOSP1-003978|   2021-07-09|      Inpatient|  PROV0330|     DEPT009|        49301|  2023-06-10|  2021-12-11|\n|  ENC000015|HOSP1-003440|   2021-09-03|   Telemedicine|  PROV0116|     DEPT018|        67685|  2024-09-14|  2020-01-07|\n|  ENC000016|HOSP1-002112|   2020-10-03|      Inpatient|  PROV0083|     DEPT010|        76228|  2023-05-24|  2024-04-11|\n|  ENC000017|HOSP1-000106|   2023-09-21|Routine Checkup|  PROV0151|     DEPT004|        92027|  2022-08-22|  2020-05-12|\n|  ENC000018|HOSP1-002499|   2021-12-17|Routine Checkup|  PROV0113|     DEPT020|        35802|  2023-04-20|  2021-05-25|\n|  ENC000019|HOSP1-001758|   2023-03-11|   Telemedicine|  PROV0250|     DEPT008|        44240|  2020-08-06|  2024-01-09|\n|  ENC000020|HOSP1-004648|   2022-12-28|      Inpatient|  PROV0391|     DEPT001|        64565|  2021-10-05|  2024-05-23|\n+-----------+------------+-------------+---------------+----------+------------+-------------+------------+------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "MYSQL_CONFIG = {\n\"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\",\n\"driver\": \"com.mysql.cj.jdbc.Driver\",\n\"user\": \"user1\",\n\"password\": \"User1-1234\",\n\"socketFactory\": \"com.google.cloud.sql.mysql.SocketFactory\",\n\"cloudSqlInstance\": \"test-project-1857:us-central1:hospital-a-mysql-db\",\n\"useSSL\": \"false\",\n \"database\": \"hospital_a_db\"    \n}\n\ndf = spark.read.format(\"jdbc\") \\\n        .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n        .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n        .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n        .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n        .option(\"dbtable\", \"encounters\") \\\n        .option(\"database\", MYSQL_CONFIG[\"database\"]) \\\n        .load()\n                \ndf.show()  "}, {"cell_type": "code", "execution_count": null, "id": "4941f360-8890-40a0-ade7-87f07af51105", "metadata": {}, "outputs": [], "source": "MYSQL_CONFIG = {\n    \"url\": \"jdbc:mysql://34.60.182.151:3306/hospital_a_db\",\n    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n    \"user\": \"user1\",\n    \"password\": \"User1-1234\",\n    \"socketFactory\": \"com.google.cloud.sql.mysql.SocketFactory\",\n    \"cloudSqlInstance\": \"test-project-1857:us-central1:hospital-a-mysql-db\",\n    \"useSSL\": \"false\",\n    \"database\": \"hospital_a_db\"\n}\n\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n    .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n    .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n    .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n    .option(\"socketFactory\", MYSQL_CONFIG[\"socketFactory\"]) \\\n    .option(\"cloudSqlInstance\", MYSQL_CONFIG[\"cloudSqlInstance\"]) \\\n    .option(\"useSSL\", MYSQL_CONFIG[\"useSSL\"]) \\\n    .option(\"dbtable\", \"encounters\") \\\n    .option(\"database\", MYSQL_CONFIG[\"database\"]) \\\n    .load()\n\ndf.show(20, False)\n"}, {"cell_type": "code", "execution_count": 13, "id": "e2196cf7-8a44-4b89-9985-149316d47094", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "10.128.0.15 \n"}], "source": "!hostname -I"}, {"cell_type": "code", "execution_count": 14, "id": "fb8d6758-7120-4368-b4e7-354d51365ef2", "metadata": {}, "outputs": [{"data": {"text/plain": "'2'"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "spark.conf.get(\"spark.executor.instances\")"}, {"cell_type": "code", "execution_count": 15, "id": "b7a37365-708c-40c6-9744-c5d6c81109f8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Driver IP:\n10.128.0.15 \nExecutor IP seen by Spark:\n"}, {"data": {"text/plain": "['10.128.0.16']"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "print(\"Driver IP:\")\n!hostname -I\n\nprint(\"Executor IP seen by Spark:\")\nspark.range(1).rdd.map(lambda x: __import__('socket').gethostbyname(__import__('socket').gethostname())).collect()\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}