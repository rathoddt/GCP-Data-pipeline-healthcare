{"cells": [{"cell_type": "code", "execution_count": 1, "id": "be484dae-9d7f-4a41-a886-23abe9639ab0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import input_file_name, when\n\n# Create Spark session\nspark = SparkSession.builder \\\n                    .appName(\"Healthcare Claims Ingestion\") \\\n                    .getOrCreate()\n\n\n# configure variables\nBUCKET_NAME = \"test-project-1857-de\"\nCLAIMS_BUCKET_PATH = f\"gs://{BUCKET_NAME}/landing/claims/*.csv\"\nBQ_TABLE = \"test-project-1857.bronze_dataset.claims\"\nTEMP_GCS_BUCKET = f\"{BUCKET_NAME}/temp/\"\n\n# read from claims source\nclaims_df = spark.read.csv(CLAIMS_BUCKET_PATH, header=True)\n\n# adding hospital source for future reference\nclaims_df = (claims_df\n                .withColumn(\"datasource\", \n                              when(input_file_name().contains(\"hospital2\"), \"hosb\")\n                             .when(input_file_name().contains(\"hospital1\"), \"hosa\").otherwise(\"None\")))\n\n# dropping dupplicates if any\nclaims_df = claims_df.dropDuplicates()\n\n# write to bigquery\n(claims_df.write\n            .format(\"bigquery\")\n            .option(\"table\", BQ_TABLE)\n            .option(\"temporaryGcsBucket\", TEMP_GCS_BUCKET)\n            .mode(\"overwrite\")\n            .save())"}, {"cell_type": "code", "execution_count": null, "id": "54a3225b-5763-4d11-9c3f-8d9c40f02985", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}